{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b917aab-5a4b-4882-aac2-e37b7d5e0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78a6926f-6cce-469b-97ac-fc2b0d311b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"\"\n",
    "\n",
    "adm_path = os.path.join(DATA_DIR, \"ADMISSIONS.csv.gz\")\n",
    "icu_path = os.path.join(DATA_DIR, \"ICUSTAYS.csv.gz\")\n",
    "patients_path = os.path.join(DATA_DIR, \"PATIENTS.csv.gz\")\n",
    "\n",
    "adm = pd.read_csv(adm_path, compression=\"gzip\")\n",
    "icu = pd.read_csv(icu_path, compression=\"gzip\")\n",
    "patients = pd.read_csv(patients_path, compression=\"gzip\")\n",
    "\n",
    "adm[\"ADMITTIME\"] = pd.to_datetime(adm[\"ADMITTIME\"], errors=\"coerce\")\n",
    "adm[\"DISCHTIME\"] = pd.to_datetime(adm[\"DISCHTIME\"], errors=\"coerce\")\n",
    "icu[\"INTIME\"] = pd.to_datetime(icu[\"INTIME\"], errors=\"coerce\")\n",
    "icu[\"OUTTIME\"] = pd.to_datetime(icu[\"OUTTIME\"], errors=\"coerce\")\n",
    "\n",
    "adm[\"MORTALITY\"] = adm[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "796199a9-b615-4393-88a5-46f35e6bf0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = icu.merge(\n",
    "    adm[[\"SUBJECT_ID\",\"HADM_ID\",\"MORTALITY\",\"ADMITTIME\"]],\n",
    "    on=[\"SUBJECT_ID\",\"HADM_ID\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "cohort = cohort.sort_values([\"SUBJECT_ID\",\"HADM_ID\",\"INTIME\"])\n",
    "\n",
    "cohort_first = cohort.groupby([\"SUBJECT_ID\",\"HADM_ID\"]).first().reset_index()\n",
    "\n",
    "cohort_key = cohort_first[[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\",\"INTIME\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aa89953-751f-4e6a-ab29-251a36017c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = cohort_first.merge(\n",
    "    adm[[\n",
    "        \"SUBJECT_ID\",\"HADM_ID\",\n",
    "        \"ADMISSION_TYPE\",\"ADMISSION_LOCATION\",\n",
    "        \"DISCHARGE_LOCATION\",\n",
    "        \"INSURANCE\",\"MARITAL_STATUS\",\"ETHNICITY\"\n",
    "    ]],\n",
    "    on=[\"SUBJECT_ID\",\"HADM_ID\"],\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34f8423c-0fd4-485e-a6da-99c4b4dee1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ICU stays: 57786\n",
      "Vital ITEMIDs: {646, 618, 220045, 220210, 211}\n",
      "chunk 25: kept 13101 rows (total 279012)\n",
      "chunk 50: kept 6944 rows (total 563884)\n",
      "chunk 75: kept 12965 rows (total 877218)\n",
      "chunk 100: kept 9882 rows (total 1196853)\n",
      "chunk 125: kept 4435 rows (total 1432415)\n",
      "chunk 150: kept 4615 rows (total 1541671)\n",
      "chunk 175: kept 3717 rows (total 1643698)\n",
      "chunk 200: kept 3476 rows (total 1758717)\n",
      "Reached MAX_CHUNKS=200, stopping early.\n",
      "chart_events shape: (1758717, 3)\n",
      "   ICUSTAY_ID  ITEMID  VALUENUM\n",
      "0    241249.0  220045      86.0\n",
      "1    241249.0  220210      21.0\n",
      "2    241249.0  220045      85.0\n",
      "3    241249.0  220210      19.0\n",
      "4    241249.0  220045      87.0\n",
      "chart_wide: (27104, 16)\n",
      "   ICUSTAY_ID  VITAL_HEART_RATE_MAX  VITAL_RESPIRATORY_RATE_MAX  \\\n",
      "0    200001.0                   NaN                         NaN   \n",
      "1    200010.0                   NaN                         NaN   \n",
      "2    200011.0                   NaN                         NaN   \n",
      "3    200016.0                   NaN                         NaN   \n",
      "4    200021.0                   NaN                         NaN   \n",
      "\n",
      "   VITAL_SPO2_MAX  VITAL_HEART_RATE_MAX  VITAL_RESPIRATORY_RATE_MAX  \\\n",
      "0             NaN                 134.0                        32.0   \n",
      "1             NaN                 115.0                        23.0   \n",
      "2             NaN                  80.0                        34.0   \n",
      "3             NaN                  91.0                        20.0   \n",
      "4             NaN                 105.0                        43.0   \n",
      "\n",
      "   VITAL_HEART_RATE_MEAN  VITAL_RESPIRATORY_RATE_MEAN  VITAL_SPO2_MEAN  \\\n",
      "0                    NaN                          NaN              NaN   \n",
      "1                    NaN                          NaN              NaN   \n",
      "2                    NaN                          NaN              NaN   \n",
      "3                    NaN                          NaN              NaN   \n",
      "4                    NaN                          NaN              NaN   \n",
      "\n",
      "   VITAL_HEART_RATE_MEAN  VITAL_RESPIRATORY_RATE_MEAN  VITAL_HEART_RATE_MIN  \\\n",
      "0             100.760000                    21.230769                   NaN   \n",
      "1              95.869565                    14.043478                   NaN   \n",
      "2              71.000000                    25.680000                   NaN   \n",
      "3              67.185185                    13.961538                   NaN   \n",
      "4              85.840000                    17.884615                   NaN   \n",
      "\n",
      "   VITAL_RESPIRATORY_RATE_MIN  VITAL_SPO2_MIN  VITAL_HEART_RATE_MIN  \\\n",
      "0                         NaN             NaN                  88.0   \n",
      "1                         NaN             NaN                  82.0   \n",
      "2                         NaN             NaN                  59.0   \n",
      "3                         NaN             NaN                  59.0   \n",
      "4                         NaN             NaN                  65.0   \n",
      "\n",
      "   VITAL_RESPIRATORY_RATE_MIN  \n",
      "0                        15.0  \n",
      "1                         6.0  \n",
      "2                        18.0  \n",
      "3                         9.0  \n",
      "4                        11.0  \n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# NEW MEMORY-SAFE VITALS BLOCK\n",
    "# =============================\n",
    "\n",
    "# 1. Map ICUSTAY_ID → INTIME (small, safe)\n",
    "icu_time_map = cohort_first[[\"ICUSTAY_ID\",\"INTIME\"]].drop_duplicates()\n",
    "icu_time_map = icu_time_map.set_index(\"ICUSTAY_ID\")[\"INTIME\"]\n",
    "icu_ids = set(icu_time_map.index)\n",
    "print(\"Number of ICU stays:\", len(icu_ids))\n",
    "\n",
    "# 2. Identify vital ITEMIDs\n",
    "d_items = pd.read_csv(os.path.join(DATA_DIR, \"D_ITEMS.csv.gz\"), compression=\"gzip\")\n",
    "d_items[\"LABEL_UP\"] = d_items[\"LABEL\"].str.upper()\n",
    "\n",
    "target_vitals = {\n",
    "    \"HEART RATE\",\n",
    "    \"SYSTOLIC BLOOD PRESSURE\",\n",
    "    \"DIASTOLIC BLOOD PRESSURE\",\n",
    "    \"RESPIRATORY RATE\",\n",
    "    \"TEMPERATURE\",\n",
    "    \"SPO2\",\n",
    "    \"O2 SATURATION\",\n",
    "    \"GCS\"\n",
    "}\n",
    "\n",
    "vital_items = d_items[d_items[\"LABEL_UP\"].isin(target_vitals)]\n",
    "vital_itemids = set(vital_items[\"ITEMID\"])\n",
    "print(\"Vital ITEMIDs:\", vital_itemids)\n",
    "\n",
    "# 3. Stream CHARTEVENTS in chunks\n",
    "chart_path = os.path.join(DATA_DIR, \"CHARTEVENTS.csv.gz\")  # change extension if needed\n",
    "usecols = [\"ICUSTAY_ID\",\"ITEMID\",\"CHARTTIME\",\"VALUENUM\"]\n",
    "\n",
    "kept = []\n",
    "total_kept = 0\n",
    "chunk_idx = 0\n",
    "\n",
    "MAX_CHUNKS = 200          # <- hard cap on how many chunks we read\n",
    "MAX_ROWS_KEPT = 2_000_000 # <- optional cap on how many rows we keep\n",
    "\n",
    "for chunk in pd.read_csv(chart_path, usecols=usecols, chunksize=300_000):\n",
    "    chunk_idx += 1\n",
    "    if chunk_idx > MAX_CHUNKS:\n",
    "        print(f\"Reached MAX_CHUNKS={MAX_CHUNKS}, stopping early.\")\n",
    "        break\n",
    "\n",
    "    # Filter early: only our ICU stays + vital itemids\n",
    "    sub = chunk[\n",
    "        chunk[\"ICUSTAY_ID\"].isin(icu_ids) &\n",
    "        chunk[\"ITEMID\"].isin(vital_itemids)\n",
    "    ].copy()\n",
    "\n",
    "    if sub.empty:\n",
    "        if chunk_idx % 25 == 0:\n",
    "            print(f\"chunk {chunk_idx}: kept 0 rows (total {total_kept})\")\n",
    "        continue\n",
    "\n",
    "    sub[\"CHARTTIME\"] = pd.to_datetime(sub[\"CHARTTIME\"], errors=\"coerce\")\n",
    "    sub[\"INTIME\"] = sub[\"ICUSTAY_ID\"].map(icu_time_map)\n",
    "    sub[\"HOUR\"] = (sub[\"CHARTTIME\"] - sub[\"INTIME\"]).dt.total_seconds()/3600\n",
    "\n",
    "    sub = sub[(sub[\"HOUR\"] >= 0) & (sub[\"HOUR\"] <= 24)]\n",
    "\n",
    "    if not sub.empty:\n",
    "        kept.append(sub[[\"ICUSTAY_ID\",\"ITEMID\",\"VALUENUM\"]])\n",
    "        total_kept += len(sub)\n",
    "\n",
    "    if chunk_idx % 25 == 0:\n",
    "        print(f\"chunk {chunk_idx}: kept {len(sub)} rows (total {total_kept})\")\n",
    "\n",
    "    if total_kept >= MAX_ROWS_KEPT:\n",
    "        print(f\"Reached MAX_ROWS_KEPT={MAX_ROWS_KEPT}, stopping early.\")\n",
    "        break\n",
    "\n",
    "\n",
    "chart_events = pd.concat(kept, ignore_index=True) if kept else pd.DataFrame(\n",
    "    columns=[\"ICUSTAY_ID\",\"ITEMID\",\"VALUENUM\"]\n",
    ")\n",
    "\n",
    "print(\"chart_events shape:\", chart_events.shape)\n",
    "print(chart_events.head())\n",
    "\n",
    "# 4. Aggregate to wide vitals feature table\n",
    "chart_agg = chart_events.groupby([\"ICUSTAY_ID\",\"ITEMID\"])[\"VALUENUM\"] \\\n",
    "                        .agg([\"mean\",\"min\",\"max\"]) \\\n",
    "                        .reset_index()\n",
    "\n",
    "itemid_to_label = dict(zip(vital_items[\"ITEMID\"], vital_items[\"LABEL_UP\"]))\n",
    "\n",
    "pivot = chart_agg.pivot_table(\n",
    "    index=\"ICUSTAY_ID\",\n",
    "    columns=\"ITEMID\",\n",
    "    values=[\"mean\",\"min\",\"max\"]\n",
    ")\n",
    "\n",
    "pivot.columns = [\n",
    "    f\"VITAL_{itemid_to_label.get(item,'UNK').replace(' ','_').upper()}_{stat.upper()}\"\n",
    "    for stat, item in pivot.columns\n",
    "]\n",
    "\n",
    "chart_wide = pivot.reset_index()\n",
    "print(\"chart_wide:\", chart_wide.shape)\n",
    "print(chart_wide.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da7864cb-1d93-4e3d-b4ed-e7f0e02b189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv(os.path.join(DATA_DIR, \"OUTPUTEVENTS.csv.gz\"), compression=\"gzip\",\n",
    "                     usecols=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\",\"CHARTTIME\",\"VALUE\"])\n",
    "\n",
    "output[\"CHARTTIME\"] = pd.to_datetime(output[\"CHARTTIME\"], errors=\"coerce\")\n",
    "output = output[output[\"HADM_ID\"].isin(cohort_key[\"HADM_ID\"])]\n",
    "\n",
    "output = output.merge(cohort_key, on=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\"], how=\"inner\")\n",
    "output[\"HOUR\"] = (output[\"CHARTTIME\"] - output[\"INTIME\"]).dt.total_seconds()/3600\n",
    "output = output[(output[\"HOUR\"]>=0) & (output[\"HOUR\"]<=24)]\n",
    "\n",
    "urine_agg = output.groupby(\"ICUSTAY_ID\")[\"VALUE\"].agg([\"sum\",\"mean\"]).reset_index()\n",
    "urine_agg.columns = [\"ICUSTAY_ID\",\"URINE_SUM_24H\",\"URINE_MEAN_24H\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6d916e7-04f6-43a2-b18e-8dd50a688c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cv = pd.read_csv(os.path.join(DATA_DIR,\"INPUTEVENTS_CV.csv.gz\"), compression=\"gzip\",\n",
    "                       usecols=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\",\"ITEMID\",\"CHARTTIME\",\"AMOUNT\"])\n",
    "input_mv = pd.read_csv(os.path.join(DATA_DIR,\"INPUTEVENTS_MV.csv.gz\"), compression=\"gzip\",\n",
    "                       usecols=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\",\"ITEMID\",\"STARTTIME\",\"AMOUNT\"])\n",
    "\n",
    "input_cv[\"CHARTTIME\"] = pd.to_datetime(input_cv[\"CHARTTIME\"], errors=\"coerce\")\n",
    "input_mv[\"STARTTIME\"] = pd.to_datetime(input_mv[\"STARTTIME\"], errors=\"coerce\")\n",
    "\n",
    "input_cv = input_cv[input_cv[\"HADM_ID\"].isin(cohort_key[\"HADM_ID\"])]\n",
    "input_mv = input_mv[input_mv[\"HADM_ID\"].isin(cohort_key[\"HADM_ID\"])]\n",
    "\n",
    "input_cv = input_cv.merge(cohort_key, on=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\"])\n",
    "input_mv = input_mv.merge(cohort_key, on=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\"])\n",
    "\n",
    "input_cv[\"HOUR\"] = (input_cv[\"CHARTTIME\"] - input_cv[\"INTIME\"]).dt.total_seconds()/3600\n",
    "input_mv[\"HOUR\"] = (input_mv[\"STARTTIME\"] - input_mv[\"INTIME\"]).dt.total_seconds()/3600\n",
    "\n",
    "input_cv = input_cv[(input_cv[\"HOUR\"]>=0)&(input_cv[\"HOUR\"]<=24)]\n",
    "input_mv = input_mv[(input_mv[\"HOUR\"]>=0)&(input_mv[\"HOUR\"]<=24)]\n",
    "\n",
    "fluids = pd.concat([\n",
    "    input_cv[[\"ICUSTAY_ID\",\"AMOUNT\"]],\n",
    "    input_mv[[\"ICUSTAY_ID\",\"AMOUNT\"]]\n",
    "])\n",
    "\n",
    "fluids_agg = fluids.groupby(\"ICUSTAY_ID\")[\"AMOUNT\"].agg([\"sum\",\"mean\"]).reset_index()\n",
    "fluids_agg.columns = [\"ICUSTAY_ID\",\"FLUID_SUM_24H\",\"FLUID_MEAN_24H\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "348aac54-9631-41f7-86ed-2ebf5b53ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = pd.read_csv(os.path.join(DATA_DIR,\"DIAGNOSES_ICD.csv.gz\"), compression=\"gzip\")\n",
    "\n",
    "def root(code):\n",
    "    if pd.isna(code): return None\n",
    "    return str(code).replace(\".\",\"\")[:3]\n",
    "\n",
    "diag[\"ICD3\"] = diag[\"ICD9_CODE\"].apply(root)\n",
    "\n",
    "prefixes = {\n",
    "    \"CMI_MI\": {\"410\",\"412\"},\n",
    "    \"CMI_CHF\": {\"428\"},\n",
    "    \"CMI_COPD\": {\"490\",\"491\",\"492\",\"494\",\"496\"},\n",
    "    \"CMI_DIAB\": {\"250\"},\n",
    "    \"CMI_RENAL\": {\"585\"},\n",
    "    \"CMI_LIVER\": {\"571\"},\n",
    "}\n",
    "\n",
    "def map_flags(icd):\n",
    "    out = {}\n",
    "    for name, pref in prefixes.items():\n",
    "        out[name] = int(icd in pref)\n",
    "    return pd.Series(out)\n",
    "\n",
    "diag_flags = diag[\"ICD3\"].apply(map_flags)\n",
    "diag_with_flags = pd.concat([diag[\"HADM_ID\"], diag_flags], axis=1)\n",
    "comorb = diag_with_flags.groupby(\"HADM_ID\").max().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d07f939c-b173-49dd-8556-731d6c5d4204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merges: (57786, 45)\n",
      "Dropping datetime columns: ['INTIME', 'OUTTIME', 'ADMITTIME']\n",
      "After cleaning: (57786, 36)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>DBSOURCE</th>\n",
       "      <th>FIRST_CAREUNIT</th>\n",
       "      <th>LAST_CAREUNIT</th>\n",
       "      <th>FIRST_WARDID</th>\n",
       "      <th>LAST_WARDID</th>\n",
       "      <th>LOS</th>\n",
       "      <th>...</th>\n",
       "      <th>URINE_SUM_24H</th>\n",
       "      <th>URINE_MEAN_24H</th>\n",
       "      <th>FLUID_SUM_24H</th>\n",
       "      <th>FLUID_MEAN_24H</th>\n",
       "      <th>CMI_MI</th>\n",
       "      <th>CMI_CHF</th>\n",
       "      <th>CMI_COPD</th>\n",
       "      <th>CMI_DIAB</th>\n",
       "      <th>CMI_RENAL</th>\n",
       "      <th>CMI_LIVER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>1</td>\n",
       "      <td>243653</td>\n",
       "      <td>carevue</td>\n",
       "      <td>NICU</td>\n",
       "      <td>NICU</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0918</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>2</td>\n",
       "      <td>211552</td>\n",
       "      <td>carevue</td>\n",
       "      <td>MICU</td>\n",
       "      <td>MICU</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>6.0646</td>\n",
       "      <td>...</td>\n",
       "      <td>497.0</td>\n",
       "      <td>38.230769</td>\n",
       "      <td>15866.802737</td>\n",
       "      <td>82.639598</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>185777</td>\n",
       "      <td>3</td>\n",
       "      <td>294638</td>\n",
       "      <td>carevue</td>\n",
       "      <td>MICU</td>\n",
       "      <td>MICU</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>1.6785</td>\n",
       "      <td>...</td>\n",
       "      <td>2150.0</td>\n",
       "      <td>537.500000</td>\n",
       "      <td>3165.000000</td>\n",
       "      <td>166.578947</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>178980</td>\n",
       "      <td>4</td>\n",
       "      <td>214757</td>\n",
       "      <td>carevue</td>\n",
       "      <td>NICU</td>\n",
       "      <td>NICU</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0844</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>107064</td>\n",
       "      <td>5</td>\n",
       "      <td>228232</td>\n",
       "      <td>carevue</td>\n",
       "      <td>SICU</td>\n",
       "      <td>SICU</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>3.6729</td>\n",
       "      <td>...</td>\n",
       "      <td>2095.0</td>\n",
       "      <td>72.241379</td>\n",
       "      <td>13315.000000</td>\n",
       "      <td>154.825581</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID  ROW_ID  ICUSTAY_ID DBSOURCE FIRST_CAREUNIT  \\\n",
       "0           2   163353       1      243653  carevue           NICU   \n",
       "1           3   145834       2      211552  carevue           MICU   \n",
       "2           4   185777       3      294638  carevue           MICU   \n",
       "3           5   178980       4      214757  carevue           NICU   \n",
       "4           6   107064       5      228232  carevue           SICU   \n",
       "\n",
       "  LAST_CAREUNIT  FIRST_WARDID  LAST_WARDID     LOS  ...  URINE_SUM_24H  \\\n",
       "0          NICU            56           56  0.0918  ...            NaN   \n",
       "1          MICU            12           12  6.0646  ...          497.0   \n",
       "2          MICU            52           52  1.6785  ...         2150.0   \n",
       "3          NICU            56           56  0.0844  ...            NaN   \n",
       "4          SICU            33           33  3.6729  ...         2095.0   \n",
       "\n",
       "  URINE_MEAN_24H FLUID_SUM_24H FLUID_MEAN_24H CMI_MI CMI_CHF CMI_COPD  \\\n",
       "0            NaN           NaN            NaN      0       0        0   \n",
       "1      38.230769  15866.802737      82.639598      1       1        0   \n",
       "2     537.500000   3165.000000     166.578947      0       0        0   \n",
       "3            NaN           NaN            NaN      0       0        0   \n",
       "4      72.241379  13315.000000     154.825581      0       0        0   \n",
       "\n",
       "   CMI_DIAB  CMI_RENAL  CMI_LIVER  \n",
       "0         0          0          0  \n",
       "1         0          0          0  \n",
       "2         0          0          1  \n",
       "3         0          0          0  \n",
       "4         0          0          0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- ORIGINAL MERGES ---\n",
    "data_full = data_base.copy()\n",
    "data_full = data_full.merge(chart_wide, on=\"ICUSTAY_ID\", how=\"left\")\n",
    "data_full = data_full.merge(urine_agg, on=\"ICUSTAY_ID\", how=\"left\")\n",
    "data_full = data_full.merge(fluids_agg, on=\"ICUSTAY_ID\", how=\"left\")\n",
    "data_full = data_full.merge(comorb, on=\"HADM_ID\",  how=\"left\")\n",
    "\n",
    "print(\"After merges:\", data_full.shape)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) FORCE ALL COLUMN NAMES TO STRINGS (fixes MultiIndex issues)\n",
    "# ===============================================================\n",
    "data_full.columns = [str(c) for c in data_full.columns]\n",
    "\n",
    "# ===============================================================\n",
    "# 2) DROP ALL DATETIME COLUMNS (massively important)\n",
    "# ===============================================================\n",
    "datetime_cols = data_full.select_dtypes(\n",
    "    include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]\n",
    ").columns.tolist()\n",
    "\n",
    "print(\"Dropping datetime columns:\", datetime_cols)\n",
    "data_full = data_full.drop(columns=datetime_cols)\n",
    "\n",
    "# ===============================================================\n",
    "# 3) DROP DUPLICATE COLUMNS (caused by merges)\n",
    "# ===============================================================\n",
    "data_full = data_full.loc[:, ~data_full.columns.duplicated()]\n",
    "\n",
    "print(\"After cleaning:\", data_full.shape)\n",
    "data_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b84480a-571e-4b5d-946e-2fff27555f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime columns (will be dropped from features): []\n",
      "Dropping from features: ['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'DISCHARGE_LOCATION']\n",
      "Number of candidate feature columns: 31\n",
      "X_train_raw: (46228, 31)\n",
      "X_test_raw: (11558, 31)\n",
      "y_train mean: 0.1005883879899628\n",
      "y_test mean: 0.10062294514621907\n",
      "Categorical columns: ['DBSOURCE', 'FIRST_CAREUNIT', 'LAST_CAREUNIT', 'ADMISSION_TYPE', 'ADMISSION_LOCATION', 'INSURANCE', 'MARITAL_STATUS', 'ETHNICITY']\n",
      "Numeric columns count: 23\n",
      "After encoding + imputation:\n",
      "X_train: (46228, 96)\n",
      "X_test: (11558, 96)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 0) Force all column names to strings\n",
    "data_full.columns = [str(c) for c in data_full.columns]\n",
    "\n",
    "# 1) Identify datetime columns and drop them from features\n",
    "datetime_cols = data_full.select_dtypes(\n",
    "    include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]\n",
    ").columns.tolist()\n",
    "print(\"Datetime columns (will be dropped from features):\", datetime_cols)\n",
    "\n",
    "# 2) Define target and columns to drop\n",
    "target_col = \"MORTALITY\"\n",
    "\n",
    "id_cols = [\"SUBJECT_ID\", \"HADM_ID\", \"ICUSTAY_ID\"]\n",
    "leakage_cols = [\"DISCHARGE_LOCATION\"]   # if not present, it's ignored\n",
    "\n",
    "drop_cols = [c for c in id_cols + leakage_cols + datetime_cols if c in data_full.columns]\n",
    "print(\"Dropping from features:\", drop_cols)\n",
    "\n",
    "# 3) Feature columns = everything except ids, leakage, datetime, target\n",
    "feature_cols = [c for c in data_full.columns if c not in drop_cols + [target_col]]\n",
    "print(\"Number of candidate feature columns:\", len(feature_cols))\n",
    "\n",
    "# 4) Build X_raw, y\n",
    "X_raw = data_full[feature_cols].copy()\n",
    "y = data_full[target_col].astype(int)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train_raw:\", X_train_raw.shape)\n",
    "print(\"X_test_raw:\", X_test_raw.shape)\n",
    "print(\"y_train mean:\", y_train.mean())\n",
    "print(\"y_test mean:\", y_test.mean())\n",
    "\n",
    "# 5) Categorical vs numeric\n",
    "cat_cols = X_train_raw.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = [c for c in X_train_raw.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Numeric columns count:\", len(num_cols))\n",
    "\n",
    "# 6) One-hot encode categoricals\n",
    "X_train = pd.get_dummies(X_train_raw, columns=cat_cols, drop_first=True)\n",
    "X_test  = pd.get_dummies(X_test_raw,  columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Align columns\n",
    "X_train, X_test = X_train.align(X_test, join=\"left\", axis=1)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "# 7) Impute numeric missing with median\n",
    "for c in num_cols:\n",
    "    if c in X_train.columns:\n",
    "        med = X_train[c].median()\n",
    "        X_train[c] = X_train[c].fillna(med)\n",
    "        X_test[c]  = X_test[c].fillna(med)\n",
    "\n",
    "print(\"After encoding + imputation:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# 8) Scale (now ONLY numeric/bool columns, no datetime)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d25e2a5d-113b-4004-b625-08cca3431ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline AUC (all features): 0.7246818593336221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "base_clf = LogisticRegression(\n",
    "    max_iter=4000,\n",
    "    penalty=\"l2\",\n",
    "    solver=\"liblinear\"\n",
    ")\n",
    "base_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "base_pred = base_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "base_auc = roc_auc_score(y_test, base_pred)\n",
    "print(\"Baseline AUC (all features):\", base_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ce2e92e-4ec5-4d4b-813e-951690cd84b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of constant columns: 0\n",
      "After dropping constant cols:\n",
      "X_train_fs: (46228, 96)\n",
      "X_test_fs: (11558, 96)\n"
     ]
    }
   ],
   "source": [
    "# Turn scaled arrays back into DataFrames\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "X_test_df  = pd.DataFrame(X_test_scaled,  columns=feature_names)\n",
    "\n",
    "# 1) Drop duplicate columns\n",
    "X_train_df = X_train_df.loc[:, ~X_train_df.columns.duplicated()]\n",
    "X_test_df  = X_test_df.loc[:, X_train_df.columns]  # ensure identical set\n",
    "\n",
    "feature_names = X_train_df.columns.tolist()\n",
    "\n",
    "# 2) Drop constant columns (vectorized for speed + correctness)\n",
    "nunique_series = X_train_df.nunique(dropna=False)\n",
    "const_cols = nunique_series[nunique_series <= 1].index.tolist()\n",
    "\n",
    "print(\"Number of constant columns:\", len(const_cols))\n",
    "\n",
    "X_train_fs = X_train_df.drop(columns=const_cols)\n",
    "X_test_fs  = X_test_df.drop(columns=const_cols, errors=\"ignore\")\n",
    "\n",
    "print(\"After dropping constant cols:\")\n",
    "print(\"X_train_fs:\", X_train_fs.shape)\n",
    "print(\"X_test_fs:\", X_test_fs.shape)\n",
    "\n",
    "feature_names = X_train_fs.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff09927a-9892-4bd6-bcfb-6fe68103d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features available: 96\n",
      "Candidate K values: [10, 20, 40, 60, 80, 96]\n",
      "k= 10 | CV AUC=0.7285 ± 0.0045\n",
      "k= 20 | CV AUC=0.7522 ± 0.0130\n",
      "k= 40 | CV AUC=0.7714 ± 0.0104\n",
      "k= 60 | CV AUC=0.7746 ± 0.0124\n",
      "k= 80 | CV AUC=0.7755 ± 0.0130\n",
      "k= 96 | CV AUC=0.7759 ± 0.0120\n",
      "\n",
      "Best K from CV: 96\n",
      "Best CV AUC   : 0.7759390045647165 ± 0.012023242665891968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Candidate K values to try\n",
    "max_features = X_train_fs.shape[1]\n",
    "candidate_ks = [10, 20, 40, 60, 80, max_features]  # adjust if you want\n",
    "\n",
    "print(\"Number of features available:\", max_features)\n",
    "print(\"Candidate K values:\", candidate_ks)\n",
    "\n",
    "results = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for k in candidate_ks:\n",
    "    k_eff = min(k, max_features)\n",
    "\n",
    "    mi_pipe = Pipeline([\n",
    "        (\"mi\", SelectKBest(mutual_info_classif, k=k_eff)),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=4000,\n",
    "            penalty=\"l2\",\n",
    "            solver=\"liblinear\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    cv_scores = cross_val_score(\n",
    "        mi_pipe,\n",
    "        X_train_fs,\n",
    "        y_train,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    mean_auc = cv_scores.mean()\n",
    "    std_auc = cv_scores.std()\n",
    "    results.append((k_eff, mean_auc, std_auc))\n",
    "\n",
    "    print(f\"k={k_eff:3d} | CV AUC={mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "\n",
    "# Pick best K based on mean CV AUC\n",
    "best_k, best_auc, best_std = max(results, key=lambda x: x[1])\n",
    "print(\"\\nBest K from CV:\", best_k)\n",
    "print(\"Best CV AUC   :\", best_auc, \"±\", best_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "772e241a-eb55-4f44-8bf0-fe2e0a74b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MI + LR model with k=20\n",
      "Test AUC: 0.7653065892102864\n"
     ]
    }
   ],
   "source": [
    "final_mi_pipe = Pipeline([\n",
    "    (\"mi\", SelectKBest(mutual_info_classif, k=20)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=4000,\n",
    "        penalty=\"l2\",\n",
    "        solver=\"liblinear\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "final_mi_pipe.fit(X_train_fs, y_train)\n",
    "pred_mi_test = final_mi_pipe.predict_proba(X_test_fs)[:, 1]\n",
    "auc_mi_test = roc_auc_score(y_test, pred_mi_test)\n",
    "\n",
    "print(f\"Final MI + LR model with k=20\")\n",
    "print(\"Test AUC:\", auc_mi_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6c05173-535e-4bac-bc9f-e0d7cc48d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top MI-selected features (up to 20):\n",
      "ADMISSION_TYPE_EMERGENCY: 0.0210\n",
      "ADMISSION_LOCATION_PHYS REFERRAL/NORMAL DELI: 0.0172\n",
      "URINE_SUM_24H: 0.0169\n",
      "LOS: 0.0162\n",
      "FIRST_CAREUNIT_NICU: 0.0147\n",
      "ADMISSION_TYPE_NEWBORN: 0.0144\n",
      "LAST_WARDID: 0.0132\n",
      "INSURANCE_Medicare: 0.0131\n",
      "FIRST_WARDID: 0.0128\n",
      "LAST_CAREUNIT_NICU: 0.0128\n",
      "FIRST_CAREUNIT_MICU: 0.0125\n",
      "LAST_CAREUNIT_MICU: 0.0101\n",
      "INSURANCE_Private: 0.0089\n",
      "ADMISSION_LOCATION_EMERGENCY ROOM ADMIT: 0.0087\n",
      "URINE_MEAN_24H: 0.0086\n",
      "LAST_CAREUNIT_CSRU: 0.0085\n",
      "FLUID_SUM_24H: 0.0079\n",
      "FLUID_MEAN_24H: 0.0059\n",
      "CMI_CHF: 0.0055\n",
      "CMI_LIVER: 0.0055\n"
     ]
    }
   ],
   "source": [
    "# ---- Top 20 MI features ----\n",
    "selector = final_mi_pipe.named_steps[\"mi\"]\n",
    "support_mask = selector.get_support()\n",
    "\n",
    "mi_scores_all = selector.scores_\n",
    "all_features = X_train_fs.columns\n",
    "\n",
    "selected_features = all_features[support_mask]\n",
    "selected_scores = mi_scores_all[support_mask]\n",
    "\n",
    "# Sort selected features by MI score descending\n",
    "mi_ranked = sorted(\n",
    "    zip(selected_features, selected_scores),\n",
    "    key=lambda x: x[1],a\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"\\nTop MI-selected features (up to 20):\")\n",
    "for name, score in mi_ranked[:20]:\n",
    "    print(f\"{name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84ccb607-7c6b-44bf-9cb9-2e1bfdb40dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1-selected feature count: 20\n",
      "Sample L1-selected features: ['ROW_ID', 'FIRST_WARDID', 'LAST_WARDID', 'LOS', 'VITAL_RESPIRATORY_RATE_MAX', 'VITAL_SPO2_MAX', 'VITAL_HEART_RATE_MEAN', 'VITAL_RESPIRATORY_RATE_MEAN', 'VITAL_SPO2_MEAN', 'VITAL_HEART_RATE_MIN', 'VITAL_RESPIRATORY_RATE_MIN', 'VITAL_SPO2_MIN', 'URINE_SUM_24H', 'URINE_MEAN_24H', 'FLUID_SUM_24H', 'FLUID_MEAN_24H', 'CMI_MI', 'CMI_CHF', 'CMI_COPD', 'CMI_DIAB', 'CMI_RENAL', 'CMI_LIVER', 'DBSOURCE_carevue', 'DBSOURCE_metavision', 'FIRST_CAREUNIT_CSRU', 'FIRST_CAREUNIT_MICU', 'FIRST_CAREUNIT_NICU', 'FIRST_CAREUNIT_SICU', 'FIRST_CAREUNIT_TSICU', 'LAST_CAREUNIT_CSRU']\n",
      "AUC with L1-selected features: 0.725084774783829\n"
     ]
    }
   ],
   "source": [
    "clf_l1 = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    C=1.0,\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=5000\n",
    ")\n",
    "clf_l1.fit(X_train_fs, y_train)\n",
    "\n",
    "coef = clf_l1.coef_.ravel()\n",
    "coef_series = pd.Series(coef, index=X_train_fs.columns)\n",
    "\n",
    "nonzero_features = coef_series[coef_series != 0].index.tolist()\n",
    "print(\"L1-selected feature count: 20\")\n",
    "print(\"Sample L1-selected features:\", nonzero_features[:30])\n",
    "\n",
    "X_train_l1 = X_train_fs[nonzero_features[:90]]\n",
    "X_test_l1  = X_test_fs[nonzero_features[:90]]\n",
    "\n",
    "clf_l1_small = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=1.0,\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=4000\n",
    ")\n",
    "clf_l1_small.fit(X_train_l1, y_train)\n",
    "\n",
    "pred_l1 = clf_l1_small.predict_proba(X_test_l1)[:, 1]\n",
    "auc_l1 = roc_auc_score(y_test, pred_l1)\n",
    "print(\"AUC with L1-selected features:\", auc_l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34432ba2-1216-4582-b12b-43f62ae949a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 RF features:\n",
      "LOS                                        0.121074\n",
      "URINE_MEAN_24H                             0.104842\n",
      "URINE_SUM_24H                              0.103733\n",
      "FLUID_SUM_24H                              0.093736\n",
      "FLUID_MEAN_24H                             0.090573\n",
      "ROW_ID                                     0.079156\n",
      "FIRST_WARDID                               0.030270\n",
      "LAST_WARDID                                0.029841\n",
      "CMI_DIAB                                   0.013679\n",
      "CMI_CHF                                    0.013119\n",
      "MARITAL_STATUS_MARRIED                     0.013078\n",
      "CMI_RENAL                                  0.011050\n",
      "CMI_COPD                                   0.010795\n",
      "ETHNICITY_WHITE                            0.010671\n",
      "ADMISSION_LOCATION_EMERGENCY ROOM ADMIT    0.010669\n",
      "CMI_MI                                     0.010567\n",
      "INSURANCE_Medicare                         0.010342\n",
      "MARITAL_STATUS_SINGLE                      0.010199\n",
      "MARITAL_STATUS_WIDOWED                     0.009425\n",
      "LAST_CAREUNIT_MICU                         0.008888\n",
      "dtype: float64\n",
      "Using top-K RF features: 20\n",
      "AUC with top-20 RF features: 0.7166088266690159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_fs, y_train)\n",
    "\n",
    "rf_importances = pd.Series(rf.feature_importances_, index=X_train_fs.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 RF features:\")\n",
    "print(rf_importances.head(20))\n",
    "\n",
    "K_rf = 20\n",
    "top_rf_features = rf_importances.head(K_rf).index.tolist()\n",
    "print(\"Using top-K RF features:\", K_rf)\n",
    "\n",
    "X_train_rf = X_train_fs[top_rf_features]\n",
    "X_test_rf  = X_test_fs[top_rf_features]\n",
    "\n",
    "clf_rf_fs = LogisticRegression(\n",
    "    max_iter=4000,\n",
    "    penalty=\"l2\",\n",
    "    solver=\"liblinear\"\n",
    ")\n",
    "clf_rf_fs.fit(X_train_rf, y_train)\n",
    "\n",
    "pred_rf_fs = clf_rf_fs.predict_proba(X_test_rf)[:, 1]\n",
    "auc_rf_fs = roc_auc_score(y_test, pred_rf_fs)\n",
    "print(\"AUC with top-%d RF features:\" % K_rf, auc_rf_fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "78cf1e21-19d6-44a9-89fe-8b57da93a364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline AUC (all features): 0.7246818593336221\n",
      "AUC with MI-selected features: 0.7653065892102864\n",
      "AUC with L1-selected features: 0.72466043558047\n",
      "AUC with RF-selected features: 0.7166088266690159\n"
     ]
    }
   ],
   "source": [
    "final_features = list(\n",
    "    set(top_mi_features) |\n",
    "    set(nonzero_features) |\n",
    "    set(top_rf_features)\n",
    ")\n",
    "\n",
    "print(\"Baseline AUC (all features):\", base_auc)\n",
    "print(\"AUC with MI-selected features:\", auc_mi_test)\n",
    "print(\"AUC with L1-selected features:\", auc_l1)\n",
    "print(\"AUC with RF-selected features:\", auc_rf_fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674604f8-7351-45aa-b3c0-252ba57c005b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
