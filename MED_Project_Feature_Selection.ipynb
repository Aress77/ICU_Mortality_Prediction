{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b917aab-5a4b-4882-aac2-e37b7d5e0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    XGBClassifier = None\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except ImportError:\n",
    "    LGBMClassifier = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6926f-6cce-469b-97ac-fc2b0d311b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"\"\n",
    "\n",
    "adm_path = os.path.join(DATA_DIR, \"ADMISSIONS.csv.gz\")\n",
    "icu_path = os.path.join(DATA_DIR, \"ICUSTAYS.csv.gz\")\n",
    "patients_path = os.path.join(DATA_DIR, \"PATIENTS.csv.gz\")\n",
    "\n",
    "adm = pd.read_csv(adm_path, compression=\"gzip\")\n",
    "icu = pd.read_csv(icu_path, compression=\"gzip\")\n",
    "patients = pd.read_csv(patients_path, compression=\"gzip\")\n",
    "\n",
    "adm[\"ADMITTIME\"] = pd.to_datetime(adm[\"ADMITTIME\"], errors=\"coerce\")\n",
    "adm[\"DISCHTIME\"] = pd.to_datetime(adm[\"DISCHTIME\"], errors=\"coerce\")\n",
    "icu[\"INTIME\"] = pd.to_datetime(icu[\"INTIME\"], errors=\"coerce\")\n",
    "icu[\"OUTTIME\"] = pd.to_datetime(icu[\"OUTTIME\"], errors=\"coerce\")\n",
    "\n",
    "adm[\"MORTALITY\"] = adm[\"HOSPITAL_EXPIRE_FLAG\"].astype(int)\n",
    "\n",
    "patients[\"DOB\"] = pd.to_datetime(patients[\"DOB\"], errors=\"coerce\")\n",
    "patients_age = patients[[\"SUBJECT_ID\", \"DOB\", \"GENDER\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796199a9-b615-4393-88a5-46f35e6bf0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = icu.merge(\n",
    "    adm[[\"SUBJECT_ID\",\"HADM_ID\",\"MORTALITY\",\"ADMITTIME\"]],\n",
    "    on=[\"SUBJECT_ID\",\"HADM_ID\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "cohort = cohort.sort_values([\"SUBJECT_ID\",\"HADM_ID\",\"INTIME\"])\n",
    "\n",
    "cohort_first = cohort.groupby([\"SUBJECT_ID\",\"HADM_ID\"]).first().reset_index()\n",
    "cohort_first = cohort_first.merge(patients_age, on=\"SUBJECT_ID\", how=\"left\")\n",
    "\n",
    "cohort_first[\"DOB\"] = pd.to_datetime(cohort_first[\"DOB\"], errors=\"coerce\")\n",
    "min_valid_dob = pd.Timestamp(\"1920-01-01\")  \n",
    "valid_dob_mask = (\n",
    "    cohort_first[\"DOB\"].notna() & \n",
    "    (cohort_first[\"DOB\"] >= min_valid_dob) & \n",
    "    (cohort_first[\"DOB\"] <= cohort_first[\"ADMITTIME\"]) &\n",
    "    (cohort_first[\"ADMITTIME\"] >= pd.Timestamp(\"2000-01-01\"))\n",
    ")\n",
    "\n",
    "cohort_first[\"AGE\"] = np.nan\n",
    "\n",
    "if valid_dob_mask.any():\n",
    "    valid_subset = cohort_first.loc[valid_dob_mask].copy()\n",
    "    year_diff = valid_subset[\"ADMITTIME\"].dt.year - valid_subset[\"DOB\"].dt.year\n",
    "    month_day_adjust = (\n",
    "        (valid_subset[\"ADMITTIME\"].dt.month * 100 + valid_subset[\"ADMITTIME\"].dt.day) < \n",
    "        (valid_subset[\"DOB\"].dt.month * 100 + valid_subset[\"DOB\"].dt.day)\n",
    "    ).astype(int)\n",
    "    \n",
    "    age_calc = (year_diff - month_day_adjust).clip(lower=0, upper=90)\n",
    "    cohort_first.loc[valid_dob_mask, \"AGE\"] = age_calc.values\n",
    "\n",
    "if cohort_first[\"AGE\"].isna().any():\n",
    "    median_age = cohort_first[\"AGE\"].median()\n",
    "    if pd.isna(median_age):\n",
    "        median_age = 65  \n",
    "    cohort_first[\"AGE\"] = cohort_first[\"AGE\"].fillna(median_age)\n",
    "\n",
    "cohort_first[\"AGE\"] = cohort_first[\"AGE\"].clip(lower=0, upper=90)\n",
    "cohort_key = cohort_first[[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\",\"INTIME\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa89953-751f-4e6a-ab29-251a36017c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base = cohort_first.merge(\n",
    "    adm[[\n",
    "        \"SUBJECT_ID\",\"HADM_ID\",\n",
    "        \"ADMISSION_TYPE\",\"ADMISSION_LOCATION\",\n",
    "        \"DISCHARGE_LOCATION\",\n",
    "        \"INSURANCE\",\"MARITAL_STATUS\",\"ETHNICITY\"\n",
    "    ]],\n",
    "    on=[\"SUBJECT_ID\",\"HADM_ID\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "data_base = data_base.merge(\n",
    "    cohort_first[[\"SUBJECT_ID\",\"HADM_ID\",\"AGE\",\"GENDER\"]],\n",
    "    on=[\"SUBJECT_ID\",\"HADM_ID\"],\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d16e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab ITEMIDs found: 19\n",
      "Lab chunk 50: kept 9375 rows (total 448811)\n",
      "lab_events shape: (948646, 3)\n",
      "lab_wide shape: (54198, 70)\n"
     ]
    }
   ],
   "source": [
    "lab_path = os.path.join(DATA_DIR, \"LABEVENTS.csv.gz\")\n",
    "d_labitems = pd.read_csv(os.path.join(DATA_DIR, \"D_LABITEMS.csv.gz\"), compression=\"gzip\")\n",
    "\n",
    "critical_labs = {\n",
    "    \"CREATININE\", \"LACTATE\", \"BILIRUBIN\", \"PLATELET\", \n",
    "    \"WBC\", \"HEMATOCRIT\", \"HEMOGLOBIN\", \"INR\", \"PTT\",\n",
    "    \"BUN\", \"SODIUM\", \"POTASSIUM\", \"GLUCOSE\", \"PH\"\n",
    "}\n",
    "\n",
    "d_labitems[\"LABEL_UP\"] = d_labitems[\"LABEL\"].str.upper()\n",
    "lab_items = d_labitems[d_labitems[\"LABEL_UP\"].isin(critical_labs)]\n",
    "lab_itemids = set(lab_items[\"ITEMID\"])\n",
    "print(\"Lab ITEMIDs found:\", len(lab_itemids))\n",
    "\n",
    "lab_kept = []\n",
    "lab_total_kept = 0\n",
    "lab_chunk_idx = 0\n",
    "\n",
    "for chunk in pd.read_csv(lab_path, usecols=[\"HADM_ID\", \"ITEMID\", \"CHARTTIME\", \"VALUENUM\"], chunksize=300_000):\n",
    "    lab_chunk_idx += 1\n",
    "    if lab_chunk_idx > 200:  \n",
    "        break\n",
    "    \n",
    "    sub = chunk[\n",
    "        chunk[\"HADM_ID\"].isin(cohort_key[\"HADM_ID\"]) &\n",
    "        chunk[\"ITEMID\"].isin(lab_itemids) &\n",
    "        chunk[\"VALUENUM\"].notna()\n",
    "    ].copy()\n",
    "    \n",
    "    if sub.empty:\n",
    "        continue\n",
    "    \n",
    "    sub[\"CHARTTIME\"] = pd.to_datetime(sub[\"CHARTTIME\"], errors=\"coerce\")\n",
    "    sub = sub.merge(cohort_key[[\"HADM_ID\", \"ICUSTAY_ID\", \"INTIME\"]], on=\"HADM_ID\", how=\"left\")\n",
    "    sub[\"HOUR\"] = (sub[\"CHARTTIME\"] - sub[\"INTIME\"]).dt.total_seconds()/3600\n",
    "    sub = sub[(sub[\"HOUR\"] >= 0) & (sub[\"HOUR\"] <= 24)]\n",
    "    \n",
    "    if not sub.empty:\n",
    "        lab_kept.append(sub[[\"ICUSTAY_ID\", \"ITEMID\", \"VALUENUM\"]])\n",
    "        lab_total_kept += len(sub)\n",
    "    \n",
    "    if lab_chunk_idx % 50 == 0:\n",
    "        print(f\"Lab chunk {lab_chunk_idx}: kept {len(sub)} rows (total {lab_total_kept})\")\n",
    "\n",
    "lab_events = pd.concat(lab_kept, ignore_index=True) if lab_kept else pd.DataFrame(\n",
    "    columns=[\"ICUSTAY_ID\", \"ITEMID\", \"VALUENUM\"]\n",
    ")\n",
    "\n",
    "print(\"lab_events shape:\", lab_events.shape)\n",
    "\n",
    "if not lab_events.empty:\n",
    "    lab_agg = lab_events.groupby([\"ICUSTAY_ID\", \"ITEMID\"])[\"VALUENUM\"].agg([\"mean\", \"min\", \"max\", \"std\"]).reset_index()\n",
    "    \n",
    "    lab_itemid_to_label = dict(zip(lab_items[\"ITEMID\"], lab_items[\"LABEL_UP\"]))\n",
    "    \n",
    "    lab_pivot = lab_agg.pivot_table(\n",
    "        index=\"ICUSTAY_ID\",\n",
    "        columns=\"ITEMID\",\n",
    "        values=[\"mean\", \"min\", \"max\", \"std\"]\n",
    "    )\n",
    "    \n",
    "    lab_pivot.columns = [\n",
    "        f\"LAB_{lab_itemid_to_label.get(item, 'UNK').replace(' ', '_').upper()}_{stat.upper()}\"\n",
    "        for stat, item in lab_pivot.columns\n",
    "    ]\n",
    "    \n",
    "    lab_wide = lab_pivot.reset_index()\n",
    "    print(\"lab_wide shape:\", lab_wide.shape)\n",
    "else:\n",
    "    lab_wide = pd.DataFrame(columns=[\"ICUSTAY_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8423c-0fd4-485e-a6da-99c4b4dee1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ICU stays: 57786\n",
      "Vital ITEMIDs: {646, 618, 220045, 220210, 211}\n",
      "chunk 25: kept 13101 rows (total 279012)\n",
      "chunk 50: kept 6944 rows (total 563884)\n",
      "chunk 75: kept 12965 rows (total 877218)\n",
      "chunk 100: kept 9882 rows (total 1196853)\n",
      "chunk 125: kept 4435 rows (total 1432415)\n",
      "chunk 150: kept 4615 rows (total 1541671)\n",
      "chunk 175: kept 3717 rows (total 1643698)\n",
      "chunk 200: kept 3476 rows (total 1758717)\n",
      "Reached MAX_CHUNKS=200, stopping early.\n",
      "chart_events shape: (1758717, 3)\n",
      "   ICUSTAY_ID  ITEMID  VALUENUM\n",
      "0    241249.0  220045      86.0\n",
      "1    241249.0  220210      21.0\n",
      "2    241249.0  220045      85.0\n",
      "3    241249.0  220210      19.0\n",
      "4    241249.0  220045      87.0\n",
      "chart_wide: (27104, 21)\n",
      "   ICUSTAY_ID  VITAL_HEART_RATE_MAX  VITAL_RESPIRATORY_RATE_MAX  \\\n",
      "0    200001.0                   NaN                         NaN   \n",
      "1    200010.0                   NaN                         NaN   \n",
      "2    200011.0                   NaN                         NaN   \n",
      "3    200016.0                   NaN                         NaN   \n",
      "4    200021.0                   NaN                         NaN   \n",
      "\n",
      "   VITAL_SPO2_MAX  VITAL_HEART_RATE_MAX  VITAL_RESPIRATORY_RATE_MAX  \\\n",
      "0             NaN                 134.0                        32.0   \n",
      "1             NaN                 115.0                        23.0   \n",
      "2             NaN                  80.0                        34.0   \n",
      "3             NaN                  91.0                        20.0   \n",
      "4             NaN                 105.0                        43.0   \n",
      "\n",
      "   VITAL_HEART_RATE_MEAN  VITAL_RESPIRATORY_RATE_MEAN  VITAL_SPO2_MEAN  \\\n",
      "0                    NaN                          NaN              NaN   \n",
      "1                    NaN                          NaN              NaN   \n",
      "2                    NaN                          NaN              NaN   \n",
      "3                    NaN                          NaN              NaN   \n",
      "4                    NaN                          NaN              NaN   \n",
      "\n",
      "   VITAL_HEART_RATE_MEAN  ...  VITAL_HEART_RATE_MIN  \\\n",
      "0             100.760000  ...                   NaN   \n",
      "1              95.869565  ...                   NaN   \n",
      "2              71.000000  ...                   NaN   \n",
      "3              67.185185  ...                   NaN   \n",
      "4              85.840000  ...                   NaN   \n",
      "\n",
      "   VITAL_RESPIRATORY_RATE_MIN  VITAL_SPO2_MIN  VITAL_HEART_RATE_MIN  \\\n",
      "0                         NaN             NaN                  88.0   \n",
      "1                         NaN             NaN                  82.0   \n",
      "2                         NaN             NaN                  59.0   \n",
      "3                         NaN             NaN                  59.0   \n",
      "4                         NaN             NaN                  65.0   \n",
      "\n",
      "   VITAL_RESPIRATORY_RATE_MIN  VITAL_HEART_RATE_STD  \\\n",
      "0                        15.0                   NaN   \n",
      "1                         6.0                   NaN   \n",
      "2                        18.0                   NaN   \n",
      "3                         9.0                   NaN   \n",
      "4                        11.0                   NaN   \n",
      "\n",
      "   VITAL_RESPIRATORY_RATE_STD  VITAL_SPO2_STD  VITAL_HEART_RATE_STD  \\\n",
      "0                         NaN             NaN             10.798457   \n",
      "1                         NaN             NaN             11.071275   \n",
      "2                         NaN             NaN              5.377422   \n",
      "3                         NaN             NaN              7.130849   \n",
      "4                         NaN             NaN             11.455275   \n",
      "\n",
      "   VITAL_RESPIRATORY_RATE_STD  \n",
      "0                    4.777511  \n",
      "1                    4.884840  \n",
      "2                    4.384822  \n",
      "3                    3.104587  \n",
      "4                    5.894587  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "icu_time_map = cohort_first[[\"ICUSTAY_ID\",\"INTIME\"]].drop_duplicates()\n",
    "icu_time_map = icu_time_map.set_index(\"ICUSTAY_ID\")[\"INTIME\"]\n",
    "icu_ids = set(icu_time_map.index)\n",
    "print(\"Number of ICU stays:\", len(icu_ids))\n",
    "\n",
    "d_items = pd.read_csv(os.path.join(DATA_DIR, \"D_ITEMS.csv.gz\"), compression=\"gzip\")\n",
    "d_items[\"LABEL_UP\"] = d_items[\"LABEL\"].str.upper()\n",
    "\n",
    "target_vitals = {\n",
    "    \"HEART RATE\",\n",
    "    \"SYSTOLIC BLOOD PRESSURE\",\n",
    "    \"DIASTOLIC BLOOD PRESSURE\",\n",
    "    \"RESPIRATORY RATE\",\n",
    "    \"TEMPERATURE\",\n",
    "    \"SPO2\",\n",
    "    \"O2 SATURATION\",\n",
    "    \"GCS\"\n",
    "}\n",
    "\n",
    "vital_items = d_items[d_items[\"LABEL_UP\"].isin(target_vitals)]\n",
    "vital_itemids = set(vital_items[\"ITEMID\"])\n",
    "print(\"Vital ITEMIDs:\", vital_itemids)\n",
    "\n",
    "chart_path = os.path.join(DATA_DIR, \"CHARTEVENTS.csv.gz\")\n",
    "usecols = [\"ICUSTAY_ID\",\"ITEMID\",\"CHARTTIME\",\"VALUENUM\"]\n",
    "\n",
    "kept = []\n",
    "total_kept = 0\n",
    "chunk_idx = 0\n",
    "\n",
    "MAX_CHUNKS = 200\n",
    "MAX_ROWS_KEPT = 2_000_000\n",
    "\n",
    "for chunk in pd.read_csv(chart_path, usecols=usecols, chunksize=300_000):\n",
    "    chunk_idx += 1\n",
    "    if chunk_idx > MAX_CHUNKS:\n",
    "        print(f\"Reached MAX_CHUNKS={MAX_CHUNKS}, stopping early.\")\n",
    "        break\n",
    "\n",
    "    sub = chunk[\n",
    "        chunk[\"ICUSTAY_ID\"].isin(icu_ids) &\n",
    "        chunk[\"ITEMID\"].isin(vital_itemids)\n",
    "    ].copy()\n",
    "\n",
    "    if sub.empty:\n",
    "        if chunk_idx % 25 == 0:\n",
    "            print(f\"chunk {chunk_idx}: kept 0 rows (total {total_kept})\")\n",
    "        continue\n",
    "\n",
    "    sub[\"CHARTTIME\"] = pd.to_datetime(sub[\"CHARTTIME\"], errors=\"coerce\")\n",
    "    sub[\"INTIME\"] = sub[\"ICUSTAY_ID\"].map(icu_time_map)\n",
    "    sub[\"HOUR\"] = (sub[\"CHARTTIME\"] - sub[\"INTIME\"]).dt.total_seconds()/3600\n",
    "\n",
    "    sub = sub[(sub[\"HOUR\"] >= 0) & (sub[\"HOUR\"] <= 24)]\n",
    "\n",
    "    if not sub.empty:\n",
    "        kept.append(sub[[\"ICUSTAY_ID\",\"ITEMID\",\"VALUENUM\"]])\n",
    "        total_kept += len(sub)\n",
    "\n",
    "    if chunk_idx % 25 == 0:\n",
    "        print(f\"chunk {chunk_idx}: kept {len(sub)} rows (total {total_kept})\")\n",
    "\n",
    "    if total_kept >= MAX_ROWS_KEPT:\n",
    "        print(f\"Reached MAX_ROWS_KEPT={MAX_ROWS_KEPT}, stopping early.\")\n",
    "        break\n",
    "\n",
    "\n",
    "chart_events = pd.concat(kept, ignore_index=True) if kept else pd.DataFrame(\n",
    "    columns=[\"ICUSTAY_ID\",\"ITEMID\",\"VALUENUM\"]\n",
    ")\n",
    "\n",
    "print(\"chart_events shape:\", chart_events.shape)\n",
    "print(chart_events.head())\n",
    "\n",
    "chart_agg = chart_events.groupby([\"ICUSTAY_ID\",\"ITEMID\"])[\"VALUENUM\"] \\\n",
    "                        .agg([\"mean\",\"min\",\"max\",\"std\"]) \\\n",
    "                        .reset_index()\n",
    "\n",
    "itemid_to_label = dict(zip(vital_items[\"ITEMID\"], vital_items[\"LABEL_UP\"]))\n",
    "\n",
    "pivot = chart_agg.pivot_table(\n",
    "    index=\"ICUSTAY_ID\",\n",
    "    columns=\"ITEMID\",\n",
    "    values=[\"mean\",\"min\",\"max\",\"std\"]\n",
    ")\n",
    "\n",
    "pivot.columns = [\n",
    "    f\"VITAL_{itemid_to_label.get(item,'UNK').replace(' ','_').upper()}_{stat.upper()}\"\n",
    "    for stat, item in pivot.columns\n",
    "]\n",
    "\n",
    "chart_wide = pivot.reset_index()\n",
    "print(\"chart_wide:\", chart_wide.shape)\n",
    "print(chart_wide.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da7864cb-1d93-4e3d-b4ed-e7f0e02b189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv(os.path.join(DATA_DIR, \"OUTPUTEVENTS.csv.gz\"), compression=\"gzip\",\n",
    "                     usecols=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\",\"CHARTTIME\",\"VALUE\"])\n",
    "\n",
    "output[\"CHARTTIME\"] = pd.to_datetime(output[\"CHARTTIME\"], errors=\"coerce\")\n",
    "output = output[output[\"HADM_ID\"].isin(cohort_key[\"HADM_ID\"])]\n",
    "\n",
    "output = output.merge(cohort_key, on=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\"], how=\"inner\")\n",
    "output[\"HOUR\"] = (output[\"CHARTTIME\"] - output[\"INTIME\"]).dt.total_seconds()/3600\n",
    "output = output[(output[\"HOUR\"]>=0) & (output[\"HOUR\"]<=24)]\n",
    "\n",
    "urine_agg = output.groupby(\"ICUSTAY_ID\")[\"VALUE\"].agg([\"sum\",\"mean\"]).reset_index()\n",
    "urine_agg.columns = [\"ICUSTAY_ID\",\"URINE_SUM_24H\",\"URINE_MEAN_24H\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6d916e7-04f6-43a2-b18e-8dd50a688c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cv = pd.read_csv(os.path.join(DATA_DIR,\"INPUTEVENTS_CV.csv.gz\"), compression=\"gzip\",\n",
    "                       usecols=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\",\"ITEMID\",\"CHARTTIME\",\"AMOUNT\"])\n",
    "input_mv = pd.read_csv(os.path.join(DATA_DIR,\"INPUTEVENTS_MV.csv.gz\"), compression=\"gzip\",\n",
    "                       usecols=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\",\"ITEMID\",\"STARTTIME\",\"AMOUNT\"])\n",
    "\n",
    "input_cv[\"CHARTTIME\"] = pd.to_datetime(input_cv[\"CHARTTIME\"], errors=\"coerce\")\n",
    "input_mv[\"STARTTIME\"] = pd.to_datetime(input_mv[\"STARTTIME\"], errors=\"coerce\")\n",
    "\n",
    "input_cv = input_cv[input_cv[\"HADM_ID\"].isin(cohort_key[\"HADM_ID\"])]\n",
    "input_mv = input_mv[input_mv[\"HADM_ID\"].isin(cohort_key[\"HADM_ID\"])]\n",
    "\n",
    "input_cv = input_cv.merge(cohort_key, on=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\"])\n",
    "input_mv = input_mv.merge(cohort_key, on=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\"])\n",
    "\n",
    "input_cv[\"HOUR\"] = (input_cv[\"CHARTTIME\"] - input_cv[\"INTIME\"]).dt.total_seconds()/3600\n",
    "input_mv[\"HOUR\"] = (input_mv[\"STARTTIME\"] - input_mv[\"INTIME\"]).dt.total_seconds()/3600\n",
    "\n",
    "input_cv = input_cv[(input_cv[\"HOUR\"]>=0)&(input_cv[\"HOUR\"]<=24)]\n",
    "input_mv = input_mv[(input_mv[\"HOUR\"]>=0)&(input_mv[\"HOUR\"]<=24)]\n",
    "\n",
    "fluids = pd.concat([\n",
    "    input_cv[[\"ICUSTAY_ID\",\"AMOUNT\"]],\n",
    "    input_mv[[\"ICUSTAY_ID\",\"AMOUNT\"]]\n",
    "])\n",
    "\n",
    "fluids_agg = fluids.groupby(\"ICUSTAY_ID\")[\"AMOUNT\"].agg([\"sum\",\"mean\"]).reset_index()\n",
    "fluids_agg.columns = [\"ICUSTAY_ID\",\"FLUID_SUM_24H\",\"FLUID_MEAN_24H\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "348aac54-9631-41f7-86ed-2ebf5b53ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = pd.read_csv(os.path.join(DATA_DIR,\"DIAGNOSES_ICD.csv.gz\"), compression=\"gzip\")\n",
    "\n",
    "def root(code):\n",
    "    if pd.isna(code): return None\n",
    "    return str(code).replace(\".\",\"\")[:3]\n",
    "\n",
    "diag[\"ICD3\"] = diag[\"ICD9_CODE\"].apply(root)\n",
    "\n",
    "prefixes = {\n",
    "    \"CMI_MI\": {\"410\",\"412\"},\n",
    "    \"CMI_CHF\": {\"428\"},\n",
    "    \"CMI_COPD\": {\"490\",\"491\",\"492\",\"494\",\"496\"},\n",
    "    \"CMI_DIAB\": {\"250\"},\n",
    "    \"CMI_RENAL\": {\"585\"},\n",
    "    \"CMI_LIVER\": {\"571\"},\n",
    "}\n",
    "\n",
    "def map_flags(icd):\n",
    "    out = {}\n",
    "    for name, pref in prefixes.items():\n",
    "        out[name] = int(icd in pref)\n",
    "    return pd.Series(out)\n",
    "\n",
    "diag_flags = diag[\"ICD3\"].apply(map_flags)\n",
    "diag_with_flags = pd.concat([diag[\"HADM_ID\"], diag_flags], axis=1)\n",
    "comorb = diag_with_flags.groupby(\"HADM_ID\").max().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f939c-b173-49dd-8556-731d6c5d4204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merges: (57786, 124)\n",
      "After removing duplicate columns: (57786, 87)\n",
      "After feature engineering: (57786, 93)\n",
      "Dropping datetime columns: ['INTIME', 'OUTTIME', 'ADMITTIME', 'DOB']\n",
      "After cleaning: (57786, 89)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>DBSOURCE</th>\n",
       "      <th>FIRST_CAREUNIT</th>\n",
       "      <th>LAST_CAREUNIT</th>\n",
       "      <th>FIRST_WARDID</th>\n",
       "      <th>LAST_WARDID</th>\n",
       "      <th>LOS</th>\n",
       "      <th>...</th>\n",
       "      <th>CMI_COPD</th>\n",
       "      <th>CMI_DIAB</th>\n",
       "      <th>CMI_RENAL</th>\n",
       "      <th>CMI_LIVER</th>\n",
       "      <th>URINE_FLUID_RATIO</th>\n",
       "      <th>VITAL_HEART_RATE_MEAN_TACHYCARDIA</th>\n",
       "      <th>VITAL_RESPIRATORY_RATE_MEAN_TACHYPNEA</th>\n",
       "      <th>VITAL_SPO2_MIN_HYPOXIA</th>\n",
       "      <th>HAS_URINE_DATA</th>\n",
       "      <th>HAS_FLUID_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>163353</td>\n",
       "      <td>1</td>\n",
       "      <td>243653</td>\n",
       "      <td>carevue</td>\n",
       "      <td>NICU</td>\n",
       "      <td>NICU</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0918</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>145834</td>\n",
       "      <td>2</td>\n",
       "      <td>211552</td>\n",
       "      <td>carevue</td>\n",
       "      <td>MICU</td>\n",
       "      <td>MICU</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>6.0646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031321</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>185777</td>\n",
       "      <td>3</td>\n",
       "      <td>294638</td>\n",
       "      <td>carevue</td>\n",
       "      <td>MICU</td>\n",
       "      <td>MICU</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>1.6785</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.679090</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>178980</td>\n",
       "      <td>4</td>\n",
       "      <td>214757</td>\n",
       "      <td>carevue</td>\n",
       "      <td>NICU</td>\n",
       "      <td>NICU</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0844</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>107064</td>\n",
       "      <td>5</td>\n",
       "      <td>228232</td>\n",
       "      <td>carevue</td>\n",
       "      <td>SICU</td>\n",
       "      <td>SICU</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>3.6729</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID  ROW_ID  ICUSTAY_ID DBSOURCE FIRST_CAREUNIT  \\\n",
       "0           2   163353       1      243653  carevue           NICU   \n",
       "1           3   145834       2      211552  carevue           MICU   \n",
       "2           4   185777       3      294638  carevue           MICU   \n",
       "3           5   178980       4      214757  carevue           NICU   \n",
       "4           6   107064       5      228232  carevue           SICU   \n",
       "\n",
       "  LAST_CAREUNIT  FIRST_WARDID  LAST_WARDID     LOS  ...  CMI_COPD CMI_DIAB  \\\n",
       "0          NICU            56           56  0.0918  ...         0        0   \n",
       "1          MICU            12           12  6.0646  ...         0        0   \n",
       "2          MICU            52           52  1.6785  ...         0        0   \n",
       "3          NICU            56           56  0.0844  ...         0        0   \n",
       "4          SICU            33           33  3.6729  ...         0        0   \n",
       "\n",
       "   CMI_RENAL CMI_LIVER URINE_FLUID_RATIO VITAL_HEART_RATE_MEAN_TACHYCARDIA  \\\n",
       "0          0         0               NaN                                 1   \n",
       "1          0         0          0.031321                                 1   \n",
       "2          0         1          0.679090                                 0   \n",
       "3          0         0               NaN                                 0   \n",
       "4          0         0          0.157330                                 0   \n",
       "\n",
       "  VITAL_RESPIRATORY_RATE_MEAN_TACHYPNEA VITAL_SPO2_MIN_HYPOXIA HAS_URINE_DATA  \\\n",
       "0                                     0                      0              0   \n",
       "1                                     0                      1              1   \n",
       "2                                     1                      0              1   \n",
       "3                                     0                      0              0   \n",
       "4                                     0                      1              1   \n",
       "\n",
       "   HAS_FLUID_DATA  \n",
       "0               0  \n",
       "1               1  \n",
       "2               1  \n",
       "3               0  \n",
       "4               1  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full = data_base.copy()\n",
    "data_full = data_full.merge(chart_wide, on=\"ICUSTAY_ID\", how=\"left\")\n",
    "data_full = data_full.merge(lab_wide, on=\"ICUSTAY_ID\", how=\"left\")  # Add lab data\n",
    "data_full = data_full.merge(urine_agg, on=\"ICUSTAY_ID\", how=\"left\")\n",
    "data_full = data_full.merge(fluids_agg, on=\"ICUSTAY_ID\", how=\"left\")\n",
    "data_full = data_full.merge(comorb, on=\"HADM_ID\",  how=\"left\")\n",
    "\n",
    "print(\"After merges:\", data_full.shape)\n",
    "data_full = data_full.loc[:, ~data_full.columns.duplicated(keep='first')]\n",
    "print(\"After removing duplicate columns:\", data_full.shape)\n",
    "\n",
    "if \"URINE_SUM_24H\" in data_full.columns and \"FLUID_SUM_24H\" in data_full.columns:\n",
    "    data_full[\"URINE_FLUID_RATIO\"] = data_full[\"URINE_SUM_24H\"] / (data_full[\"FLUID_SUM_24H\"] + 1)\n",
    "\n",
    "vital_cols = [c for c in data_full.columns if \"VITAL_\" in str(c)]\n",
    "for col in vital_cols:\n",
    "    try:\n",
    "        col_idx = data_full.columns.get_loc(col)\n",
    "        if isinstance(col_idx, slice):\n",
    "            continue\n",
    "        \n",
    "        col_data = data_full.iloc[:, col_idx]\n",
    "        if not pd.api.types.is_numeric_dtype(col_data):\n",
    "            continue\n",
    "        if \"HEART_RATE\" in str(col) and \"MEAN\" in str(col):\n",
    "            new_col_name = f\"{col}_TACHYCARDIA\"\n",
    "            if new_col_name not in data_full.columns:\n",
    "                data_full[new_col_name] = (col_data > 100).astype(int)\n",
    "        elif \"RESPIRATORY_RATE\" in str(col) and \"MEAN\" in str(col):\n",
    "            new_col_name = f\"{col}_TACHYPNEA\"\n",
    "            if new_col_name not in data_full.columns:\n",
    "                data_full[new_col_name] = (col_data > 20).astype(int)\n",
    "        elif \"SPO2\" in str(col) and \"MIN\" in str(col):\n",
    "            new_col_name = f\"{col}_HYPOXIA\"\n",
    "            if new_col_name not in data_full.columns:\n",
    "                data_full[new_col_name] = (col_data < 90).astype(int)\n",
    "    except (KeyError, ValueError, TypeError, IndexError) as e:\n",
    "        continue\n",
    "\n",
    "if \"URINE_SUM_24H\" in data_full.columns:\n",
    "    data_full[\"HAS_URINE_DATA\"] = data_full[\"URINE_SUM_24H\"].notna().astype(int)\n",
    "if \"FLUID_SUM_24H\" in data_full.columns:\n",
    "    data_full[\"HAS_FLUID_DATA\"] = data_full[\"FLUID_SUM_24H\"].notna().astype(int)\n",
    "\n",
    "print(\"After feature engineering:\", data_full.shape)\n",
    "\n",
    "data_full.columns = [str(c) for c in data_full.columns]\n",
    "\n",
    "datetime_cols = data_full.select_dtypes(\n",
    "    include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]\n",
    ").columns.tolist()\n",
    "\n",
    "print(\"Dropping datetime columns:\", datetime_cols)\n",
    "data_full = data_full.drop(columns=datetime_cols)\n",
    "\n",
    "data_full = data_full.loc[:, ~data_full.columns.duplicated()]\n",
    "\n",
    "print(\"After cleaning:\", data_full.shape)\n",
    "data_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84480a-571e-4b5d-946e-2fff27555f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime columns (will be dropped from features): []\n",
      "Dropping from features: ['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'DISCHARGE_LOCATION']\n",
      "Number of candidate feature columns: 84\n",
      "X_train_raw: (46228, 84)\n",
      "X_test_raw: (11558, 84)\n",
      "y_train mean: 0.1005883879899628\n",
      "y_test mean: 0.10062294514621907\n",
      "Categorical columns: ['DBSOURCE', 'FIRST_CAREUNIT', 'LAST_CAREUNIT', 'GENDER_x', 'ADMISSION_TYPE', 'ADMISSION_LOCATION', 'INSURANCE', 'MARITAL_STATUS', 'ETHNICITY', 'GENDER_y']\n",
      "Numeric columns count: 74\n",
      "After encoding + imputation:\n",
      "X_train: (46228, 149)\n",
      "X_test: (11558, 149)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_full.columns = [str(c) for c in data_full.columns]\n",
    "\n",
    "datetime_cols = data_full.select_dtypes(\n",
    "    include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]\n",
    ").columns.tolist()\n",
    "print(\"Datetime columns (will be dropped from features):\", datetime_cols)\n",
    "target_col = \"MORTALITY\"\n",
    "\n",
    "id_cols = [\"SUBJECT_ID\", \"HADM_ID\", \"ICUSTAY_ID\"]\n",
    "leakage_cols = [\"DISCHARGE_LOCATION\"]   \n",
    "\n",
    "drop_cols = [c for c in id_cols + leakage_cols + datetime_cols if c in data_full.columns]\n",
    "print(\"Dropping from features:\", drop_cols)\n",
    "\n",
    "feature_cols = [c for c in data_full.columns if c not in drop_cols + [target_col]]\n",
    "print(\"Number of candidate feature columns:\", len(feature_cols))\n",
    "\n",
    "X_raw = data_full[feature_cols].copy()\n",
    "y = data_full[target_col].astype(int)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_raw, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"X_train_raw:\", X_train_raw.shape)\n",
    "print(\"X_test_raw:\", X_test_raw.shape)\n",
    "print(\"y_train mean:\", y_train.mean())\n",
    "print(\"y_test mean:\", y_test.mean())\n",
    "\n",
    "cat_cols = X_train_raw.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = [c for c in X_train_raw.columns if c not in cat_cols]\n",
    "\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Numeric columns count:\", len(num_cols))\n",
    "\n",
    "X_train = pd.get_dummies(X_train_raw, columns=cat_cols, drop_first=True)\n",
    "X_test  = pd.get_dummies(X_test_raw,  columns=cat_cols, drop_first=True)\n",
    "\n",
    "X_train, X_test = X_train.align(X_test, join=\"left\", axis=1)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "for c in num_cols:\n",
    "    if c in X_train.columns:\n",
    "        med = X_train[c].median()\n",
    "        X_train[c] = X_train[c].fillna(med)\n",
    "        X_test[c]  = X_test[c].fillna(med)\n",
    "\n",
    "print(\"After encoding + imputation:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d25e2a5d-113b-4004-b625-08cca3431ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline AUC (all features): 0.4977689518532167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "base_clf = LogisticRegression(\n",
    "    max_iter=4000,\n",
    "    penalty=\"l2\",\n",
    "    solver=\"liblinear\"\n",
    ")\n",
    "base_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "base_pred = base_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "base_auc = roc_auc_score(y_test, base_pred)\n",
    "print(\"Baseline AUC (all features):\", base_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2e92e-4ec5-4d4b-813e-951690cd84b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of constant columns: 3\n",
      "After dropping constant cols:\n",
      "X_train_fs: (46228, 146)\n",
      "X_test_fs: (11558, 146)\n"
     ]
    }
   ],
   "source": [
    "X_train_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "X_test_df  = pd.DataFrame(X_test_scaled,  columns=feature_names)\n",
    "\n",
    "X_train_df = X_train_df.loc[:, ~X_train_df.columns.duplicated()]\n",
    "X_test_df  = X_test_df.loc[:, X_train_df.columns] \n",
    "\n",
    "feature_names = X_train_df.columns.tolist()\n",
    "nunique_series = X_train_df.nunique(dropna=False)\n",
    "const_cols = nunique_series[nunique_series <= 1].index.tolist()\n",
    "\n",
    "print(\"Number of constant columns:\", len(const_cols))\n",
    "\n",
    "X_train_fs = X_train_df.drop(columns=const_cols)\n",
    "X_test_fs  = X_test_df.drop(columns=const_cols, errors=\"ignore\")\n",
    "\n",
    "print(\"After dropping constant cols:\")\n",
    "print(\"X_train_fs:\", X_train_fs.shape)\n",
    "print(\"X_test_fs:\", X_test_fs.shape)\n",
    "\n",
    "feature_names = X_train_fs.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09927a-9892-4bd6-bcfb-6fe68103d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features available: 146\n",
      "Candidate K values: [10, 20, 40, 60, 80, 146]\n",
      "k= 10 | CV AUC=0.7547 ± 0.0222\n",
      "k= 20 | CV AUC=0.7856 ± 0.0115\n",
      "k= 40 | CV AUC=0.8061 ± 0.0116\n",
      "k= 60 | CV AUC=0.8189 ± 0.0110\n",
      "k= 80 | CV AUC=0.8221 ± 0.0097\n",
      "k=146 | CV AUC=0.8257 ± 0.0101\n",
      "\n",
      "Best K from CV: 146\n",
      "Best CV AUC   : 0.8256571262095544 ± 0.010115720890134202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "max_features = X_train_fs.shape[1]\n",
    "candidate_ks = [10, 20, 40, 60, 80, max_features]  # adjust if you want\n",
    "\n",
    "print(\"Number of features available:\", max_features)\n",
    "print(\"Candidate K values:\", candidate_ks)\n",
    "\n",
    "results = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for k in candidate_ks:\n",
    "    k_eff = min(k, max_features)\n",
    "\n",
    "    mi_pipe = Pipeline([\n",
    "        (\"mi\", SelectKBest(mutual_info_classif, k=k_eff)),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=4000,\n",
    "            penalty=\"l2\",\n",
    "            solver=\"liblinear\"\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    cv_scores = cross_val_score(\n",
    "        mi_pipe,\n",
    "        X_train_fs,\n",
    "        y_train,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    mean_auc = cv_scores.mean()\n",
    "    std_auc = cv_scores.std()\n",
    "    results.append((k_eff, mean_auc, std_auc))\n",
    "\n",
    "    print(f\"k={k_eff:3d} | CV AUC={mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "\n",
    "best_k, best_auc, best_std = max(results, key=lambda x: x[1])\n",
    "print(\"\\nBest K from CV:\", best_k)\n",
    "print(\"Best CV AUC   :\", best_auc, \"±\", best_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e241a-eb55-4f44-8bf0-fe2e0a74b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MI + LR model with k=146\n",
      "Test AUC: 0.4977694895149753\n"
     ]
    }
   ],
   "source": [
    "final_mi_pipe = Pipeline([\n",
    "    (\"mi\", SelectKBest(mutual_info_classif, k=best_k)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=4000,\n",
    "        penalty=\"l2\",\n",
    "        solver=\"liblinear\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "final_mi_pipe.fit(X_train_fs, y_train)\n",
    "pred_mi_test = final_mi_pipe.predict_proba(X_test_fs)[:, 1]\n",
    "auc_mi_test = roc_auc_score(y_test, pred_mi_test)\n",
    "\n",
    "print(f\"Final MI + LR model with k={best_k}\")\n",
    "print(\"Test AUC:\", auc_mi_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c05173-535e-4bac-bc9f-e0d7cc48d7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top MI-selected features (up to 20):\n",
      "LAB_PH_MIN: 0.0314\n",
      "LAB_PH_MEAN: 0.0304\n",
      "LAB_LACTATE_MIN: 0.0293\n",
      "LAB_LACTATE_MEAN: 0.0290\n",
      "LAB_LACTATE_MAX: 0.0268\n",
      "AGE_x: 0.0234\n",
      "ADMISSION_TYPE_EMERGENCY: 0.0207\n",
      "LAB_PH_MAX: 0.0205\n",
      "AGE_y: 0.0198\n",
      "URINE_FLUID_RATIO: 0.0183\n",
      "URINE_SUM_24H: 0.0173\n",
      "LAB_CREATININE_MEAN: 0.0164\n",
      "LOS: 0.0157\n",
      "LAB_CREATININE_MAX: 0.0155\n",
      "ADMISSION_LOCATION_PHYS REFERRAL/NORMAL DELI: 0.0154\n",
      "FIRST_WARDID: 0.0152\n",
      "LAST_CAREUNIT_NICU: 0.0150\n",
      "LAB_LACTATE_STD: 0.0147\n",
      "LAB_PH_STD: 0.0144\n",
      "LAB_SODIUM_MEAN: 0.0139\n"
     ]
    }
   ],
   "source": [
    "selector = final_mi_pipe.named_steps[\"mi\"]\n",
    "support_mask = selector.get_support()\n",
    "\n",
    "mi_scores_all = selector.scores_\n",
    "all_features = X_train_fs.columns\n",
    "\n",
    "selected_features = all_features[support_mask]\n",
    "selected_scores = mi_scores_all[support_mask]\n",
    "\n",
    "mi_ranked = sorted(\n",
    "    zip(selected_features, selected_scores),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "top_mi_features = [name for name, score in mi_ranked[:20]]\n",
    "\n",
    "print(\"\\nTop MI-selected features (up to 20):\")\n",
    "for name, score in mi_ranked[:20]:\n",
    "    print(f\"{name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84ccb607-7c6b-44bf-9cb9-2e1bfdb40dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1-selected feature count: 20\n",
      "Sample L1-selected features: ['ROW_ID', 'FIRST_WARDID', 'LAST_WARDID', 'LOS', 'AGE_x', 'AGE_y', 'VITAL_HEART_RATE_MAX', 'VITAL_RESPIRATORY_RATE_MAX', 'VITAL_SPO2_MAX', 'VITAL_HEART_RATE_MEAN', 'VITAL_RESPIRATORY_RATE_MEAN', 'VITAL_SPO2_MEAN', 'VITAL_HEART_RATE_MIN', 'VITAL_RESPIRATORY_RATE_MIN', 'VITAL_SPO2_MIN', 'VITAL_HEART_RATE_STD', 'VITAL_RESPIRATORY_RATE_STD', 'VITAL_SPO2_STD', 'LAB_GLUCOSE_MAX', 'LAB_HEMOGLOBIN_MAX', 'LAB_LACTATE_MAX', 'LAB_PH_MAX', 'LAB_SODIUM_MAX', 'LAB_HEMATOCRIT_MAX', 'LAB_PTT_MAX', 'LAB_WBC_MAX', 'LAB_GLUCOSE_MEAN', 'LAB_HEMOGLOBIN_MEAN', 'LAB_LACTATE_MEAN', 'LAB_PH_MEAN']\n",
      "AUC with L1-selected features: 0.49319841331879166\n"
     ]
    }
   ],
   "source": [
    "clf_l1 = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    C=1.0,\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=5000\n",
    ")\n",
    "clf_l1.fit(X_train_fs, y_train)\n",
    "\n",
    "coef = clf_l1.coef_.ravel()\n",
    "coef_series = pd.Series(coef, index=X_train_fs.columns)\n",
    "\n",
    "nonzero_features = coef_series[coef_series != 0].index.tolist()\n",
    "print(\"L1-selected feature count: 20\")\n",
    "print(\"Sample L1-selected features:\", nonzero_features[:30])\n",
    "\n",
    "X_train_l1 = X_train_fs[nonzero_features[:90]]\n",
    "X_test_l1  = X_test_fs[nonzero_features[:90]]\n",
    "\n",
    "clf_l1_small = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=1.0,\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=4000\n",
    ")\n",
    "clf_l1_small.fit(X_train_l1, y_train)\n",
    "\n",
    "pred_l1 = clf_l1_small.predict_proba(X_test_l1)[:, 1]\n",
    "auc_l1 = roc_auc_score(y_test, pred_l1)\n",
    "print(\"AUC with L1-selected features:\", auc_l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34432ba2-1216-4582-b12b-43f62ae949a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 RF features:\n",
      "LOS                    0.050504\n",
      "URINE_SUM_24H          0.033259\n",
      "LAB_LACTATE_MEAN       0.031694\n",
      "URINE_MEAN_24H         0.031123\n",
      "LAB_PH_MEAN            0.031062\n",
      "URINE_FLUID_RATIO      0.030011\n",
      "LAB_PH_MIN             0.029772\n",
      "LAB_LACTATE_MIN        0.028029\n",
      "FLUID_MEAN_24H         0.026907\n",
      "LAB_LACTATE_MAX        0.025457\n",
      "FLUID_SUM_24H          0.024705\n",
      "AGE_x                  0.022966\n",
      "AGE_y                  0.022721\n",
      "LAB_CREATININE_MEAN    0.022213\n",
      "LAB_PH_MAX             0.021725\n",
      "LAB_PTT_MIN            0.021463\n",
      "LAB_SODIUM_MEAN        0.021423\n",
      "LAB_HEMATOCRIT_MEAN    0.021154\n",
      "LAB_HEMATOCRIT_MAX     0.021007\n",
      "LAB_HEMATOCRIT_MIN     0.020743\n",
      "dtype: float64\n",
      "Using top-K RF features: 20\n",
      "AUC with top-20 RF features: 0.49249118131319336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_fs, y_train)\n",
    "\n",
    "rf_importances = pd.Series(rf.feature_importances_, index=X_train_fs.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 RF features:\")\n",
    "print(rf_importances.head(20))\n",
    "\n",
    "K_rf = 20\n",
    "top_rf_features = rf_importances.head(K_rf).index.tolist()\n",
    "print(\"Using top-K RF features:\", K_rf)\n",
    "\n",
    "X_train_rf = X_train_fs[top_rf_features]\n",
    "X_test_rf  = X_test_fs[top_rf_features]\n",
    "\n",
    "clf_rf_fs = LogisticRegression(\n",
    "    max_iter=4000,\n",
    "    penalty=\"l2\",\n",
    "    solver=\"liblinear\"\n",
    ")\n",
    "clf_rf_fs.fit(X_train_rf, y_train)\n",
    "\n",
    "pred_rf_fs = clf_rf_fs.predict_proba(X_test_rf)[:, 1]\n",
    "auc_rf_fs = roc_auc_score(y_test, pred_rf_fs)\n",
    "print(\"AUC with top-%d RF features:\" % K_rf, auc_rf_fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78cf1e21-19d6-44a9-89fe-8b57da93a364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline AUC (all features): 0.4977689518532167\n",
      "AUC with MI-selected features: 0.4977694895149753\n",
      "AUC with L1-selected features: 0.49319841331879166\n",
      "AUC with RF-selected features: 0.49249118131319336\n"
     ]
    }
   ],
   "source": [
    "final_features = list(\n",
    "    set(top_mi_features) |\n",
    "    set(nonzero_features) |\n",
    "    set(top_rf_features)\n",
    ")\n",
    "\n",
    "print(\"Baseline AUC (all features):\", base_auc)\n",
    "print(\"AUC with MI-selected features:\", auc_mi_test)\n",
    "print(\"AUC with L1-selected features:\", auc_l1)\n",
    "print(\"AUC with RF-selected features:\", auc_rf_fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408c7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance ratio: 8.94\n",
      "\n",
      "=== XGBoost Hyperparameter Tuning ===\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAHIL\\anaconda3\\envs\\dspy_env\\lib\\site-packages\\xgboost\\training.py:199: UserWarning: [16:24:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost params: {'colsample_bytree': np.float64(0.8833253661489003), 'gamma': np.float64(0.10459979618751881), 'learning_rate': np.float64(0.02442648266371311), 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 723, 'subsample': np.float64(0.7353573712051881)}\n",
      "Best XGBoost CV AUC: 0.8844205780907611\n",
      "XGBoost Test AUC (tuned): 0.807505\n",
      "\n",
      "=== LightGBM Hyperparameter Tuning ===\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best LightGBM params: {'bagging_fraction': np.float64(0.7131210262072643), 'bagging_freq': 3, 'feature_fraction': np.float64(0.7413426598703142), 'learning_rate': np.float64(0.019733837066347234), 'max_depth': 5, 'min_child_samples': 29, 'min_child_weight': np.float64(0.02568760628386012), 'n_estimators': 594, 'num_leaves': 21}\n",
      "Best LightGBM CV AUC: 0.8837980854916557\n",
      "LightGBM Test AUC (tuned): 0.747945\n",
      "\n",
      "=== Best Model on Combined Features ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAHIL\\anaconda3\\envs\\dspy_env\\lib\\site-packages\\xgboost\\training.py:199: UserWarning: [16:29:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost on Combined Features AUC: 0.791912\n"
     ]
    }
   ],
   "source": [
    "scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "print(f\"Class imbalance ratio: {scale_pos_weight:.2f}\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_tune, X_val_tune, y_train_tune, y_val_tune = train_test_split(\n",
    "    X_train_fs, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "if XGBClassifier is not None:\n",
    "    print(\"\\nXGBoost Hyperparameter Tuning\")\n",
    "    xgb_base = XGBClassifier(\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    param_dist_xgb = {\n",
    "        'n_estimators': randint(300, 800),\n",
    "        'max_depth': randint(4, 9),\n",
    "        'learning_rate': uniform(0.01, 0.15),\n",
    "        'subsample': uniform(0.6, 0.35),\n",
    "        'colsample_bytree': uniform(0.6, 0.35),\n",
    "        'min_child_weight': randint(1, 6),\n",
    "        'gamma': uniform(0, 0.3)\n",
    "    }\n",
    "    \n",
    "    random_search_xgb = RandomizedSearchCV(\n",
    "        xgb_base, param_dist_xgb, n_iter=30, cv=3,\n",
    "        scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1\n",
    "    )\n",
    "    \n",
    "    random_search_xgb.fit(X_train_tune, y_train_tune)\n",
    "    \n",
    "    print(\"Best XGBoost params:\", random_search_xgb.best_params_)\n",
    "    print(\"Best XGBoost CV AUC:\", random_search_xgb.best_score_)\n",
    "    \n",
    "    pred_xgb_tuned = random_search_xgb.predict_proba(X_test_fs)[:, 1]\n",
    "    auc_xgb_tuned = roc_auc_score(y_test, pred_xgb_tuned)\n",
    "    print(f\"XGBoost Test AUC (tuned): {auc_xgb_tuned:.6f}\")\n",
    "\n",
    "if LGBMClassifier is not None:\n",
    "    print(\"\\nLightGBM Hyperparameter Tuning\")\n",
    "    lgbm_base = LGBMClassifier(\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    param_dist_lgbm = {\n",
    "        'n_estimators': randint(300, 800),\n",
    "        'max_depth': randint(4, 9),\n",
    "        'learning_rate': uniform(0.01, 0.15),\n",
    "        'num_leaves': randint(20, 50),\n",
    "        'feature_fraction': uniform(0.6, 0.35),\n",
    "        'bagging_fraction': uniform(0.6, 0.35),\n",
    "        'bagging_freq': randint(3, 8),\n",
    "        'min_child_samples': randint(10, 30),\n",
    "        'min_child_weight': uniform(0.001, 0.1)\n",
    "    }\n",
    "    \n",
    "    random_search_lgbm = RandomizedSearchCV(\n",
    "        lgbm_base, param_dist_lgbm, n_iter=30, cv=3,\n",
    "        scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1\n",
    "    )\n",
    "    \n",
    "    random_search_lgbm.fit(X_train_tune, y_train_tune)\n",
    "    \n",
    "    print(\"Best LightGBM params:\", random_search_lgbm.best_params_)\n",
    "    print(\"Best LightGBM CV AUC:\", random_search_lgbm.best_score_)\n",
    "\n",
    "    pred_lgbm_tuned = random_search_lgbm.predict_proba(X_test_fs)[:, 1]\n",
    "    auc_lgbm_tuned = roc_auc_score(y_test, pred_lgbm_tuned)\n",
    "    print(f\"LightGBM Test AUC (tuned): {auc_lgbm_tuned:.6f}\")\n",
    "\n",
    "if XGBClassifier is not None and 'final_features' in locals():\n",
    "    print(\"\\nBest Model on Combined Features\")\n",
    "    X_train_combined = X_train_fs[final_features]\n",
    "    X_test_combined = X_test_fs[final_features]\n",
    "    \n",
    "    best_xgb = random_search_xgb.best_estimator_\n",
    "    best_xgb.fit(X_train_combined, y_train)\n",
    "    pred_combined = best_xgb.predict_proba(X_test_combined)[:, 1]\n",
    "    auc_combined = roc_auc_score(y_test, pred_combined)\n",
    "    print(f\"XGBoost on Combined Features AUC: {auc_combined:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674604f8-7351-45aa-b3c0-252ba57c005b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (dspy_env)",
   "language": "python",
   "name": "dspy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
